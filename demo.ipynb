{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.identity_lm_crypto import generate_keys_multi\n",
    "from modules.detector import IdentityDetector\n",
    "from modules.signer import IdentitySigner\n",
    "\n",
    "# Run below pip commands if you run this in colab as colab doesn't have these packages\n",
    "# !pip install pycryptodome\n",
    "# !pip install transformers\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "token_standard=8 # length of token sequence chunks that we encrypt\n",
    "arb_size=16 # Bit Size of secret primes we create in the multi prime RSA implementation\n",
    "allowed_distance=3 # how much we allow generated chunks to differ from ideal chunks in terms of token group\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_key,public_key=generate_keys_multi(n_bits=token_standard,arb_size=arb_size) # generate public and private key\n",
    "private_key,public_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models from Hugging Face\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build encoder and decoder using the keys and params\n",
    "S=IdentitySigner(private_key,public_key,allowed_distance,token_standard,model,tokenizer,device)\n",
    "D=IdentityDetector(public_key,allowed_distance,token_standard,model,tokenizer,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set prompt (or write your own!)\n",
    "prompt=\"Mission to the Unknown is the only episode of Doctor Who that doesn't feature the Doctor at all. Instead it focuses on a security agent's efforts to warn Earth about an alien attack.\"\n",
    "prompt=\"Edward Dando was a thief who overate at food stalls and inns, then revealed that he had no money to pay. He was particularly fond of oysters, once eating 300 in a sitting.\"\n",
    "# prompt=\"Atoms of radioactive elements can split. According to Albert Einstein, mass and energy are interchangeable under certain circumstances. When atoms split, the process is called nuclear fission. In this case, a small amount of mass is converted into energy. \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model expects prompts that look like this\n",
    "prompt=f\"Paraphrase: {prompt}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(prompt)>12*token_standard,\"Prompt too short\"\n",
    "assert len(prompt)<600,\"Prompt too long for this particular paraphrasing model to be effective\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sign the prompt\n",
    "signed_prompt,signed_tokens=S.rewrite(prompt)\n",
    "signed_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does the original prompt pass? This should fail >90% of the time.  \n",
    "val,truth_vals= D.detect(prompt) # val is overall pass/fail, truth_vals is pass/fail for each chunk of token_standard length tokens in the prompt\n",
    "if val:\n",
    "    print(\"This prompt was signed by the user\")\n",
    "else:\n",
    "    print(\"This prompt was not signed by the user\")\n",
    "\n",
    "print(f\"This prompt passed detection for {np.count_nonzero(truth_vals)} / {truth_vals.size} chunk tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does the encoded prompt pass? This should work >90% of the time.\n",
    "val,truth_vals= D.detect(signed_prompt) # val is overall pass/fail, truth_vals is pass/fail for each chunk of token_standard length in the prompt\n",
    "if val:\n",
    "    print(\"This prompt was signed by the user\")\n",
    "else:\n",
    "    print(\"This prompt was not signed by the user\")\n",
    "\n",
    "print(f\"This prompt passed detection for {np.count_nonzero(truth_vals)} / {truth_vals.size} chunk tests\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
