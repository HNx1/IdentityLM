{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self contained demo that can be run directly in colab\n",
    "\n",
    "# Run below as colab doesn't have transformers \n",
    "# !pip install transformers\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import numpy as np\n",
    "import itertools\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityDetector():\n",
    "    def __init__(self,public_key,allowed_distance,token_standard,model,tokenizer,device=\"cuda\",digest_size=8):\n",
    "        self.e=public_key[0] # public key exponent\n",
    "        self.n=public_key[1] # public key modulus\n",
    "        self.r=public_key[2] # public key 3rd prime\n",
    "        self.allowed_distance=allowed_distance\n",
    "        self.token_standard=token_standard\n",
    "        self.device=device\n",
    "        self.digest_size=digest_size\n",
    "        self.tokenizer=tokenizer\n",
    "        # Get vocab_size to ensure match between logit shape and vocab_size in decoder - tokenizer vocab_size does not always match the logit shape exactly.\n",
    "        self.vocab_size=[x for x in model.children()][-1].out_features\n",
    "\n",
    "    def detect(self,text):\n",
    "        # Detection function will take in text, analyze it chunk by chunk to see if the chunk hashes are similar to the expected encryption (by decrypting similar hashes)\n",
    "\n",
    "        # Tokenize the input text\n",
    "        token_ids=self.tokenizer.encode(\n",
    "            text, \n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\")\n",
    "        # Add a padding token as the generated paraphrase always starts with it\n",
    "        token_ids=torch.cat((torch.tensor([[self.tokenizer.pad_token_id]]).to(self.device),token_ids),dim=-1).to(self.device)\n",
    "        # Split into token_standard length chunks, retaining the end bit (i.e. the last token_ids.shape[-1] mod(token_standard) tokens) separately\n",
    "        chunk_count=token_ids.shape[-1] // self.token_standard\n",
    "        chunks=token_ids[0][:chunk_count*self.token_standard].reshape(chunk_count,-1)\n",
    "        remainder=token_ids[0][chunk_count*self.token_standard:]\n",
    "    \n",
    "        # Set the seen_Sequence to be the first chunk\n",
    "        seen_sequence=chunks[0]\n",
    "\n",
    "        # List holder for whether chunks pass detection or not\n",
    "        chunk_bools=[]\n",
    "        # For each chunk except the first\n",
    "        for chunk in chunks[1:]:\n",
    "\n",
    "            # Hash tokens generated with public key modulus appended to front\n",
    "            sequence_to_hash=[bytes(str(self.n),\"utf-8\")]+[bytes(str(x.item()),\"utf-8\") for x in seen_sequence]\n",
    "            h=hashlib.shake_256(b\"\".join(sequence_to_hash))\n",
    "\n",
    "            # Get a digest of a certain byte size and convert to decimal\n",
    "            digest=int(h.hexdigest(self.digest_size),16)\n",
    "\n",
    "            # Get digest mod r\n",
    "            digest= digest % self.r\n",
    "\n",
    "            # Set up the group ids\n",
    "            chunk_group_ids=[]\n",
    "            # For each token in the chunk\n",
    "            for token in chunk:\n",
    "                # Seed numpy RNG with the current hash - we get maximum digest that doesn't overflow numpy manual seeding\n",
    "                # Note this replicates exactly the encoder - given a token sequence and a public key modulus, we get exactly the same seed for numpy \n",
    "                sequence_to_hash=[bytes(str(x.item()),\"utf-8\") for x in seen_sequence]+[bytes(str(self.n),\"utf-8\")]\n",
    "                h=hashlib.shake_256(b\"\".join(sequence_to_hash))\n",
    "                numpy_seed=int(h.hexdigest(4),16)\n",
    "                np.random.seed(numpy_seed)\n",
    "\n",
    "                # Get token groups from the seeded RNG\n",
    "                token_group=torch.from_numpy(np.random.choice(self.vocab_size,self.vocab_size//2,replace=False)).to(torch.int64).to(self.device)\n",
    "\n",
    "                # Add binary character to our chunk_group_ids depending on whether the token is in the token group or not\n",
    "                chunk_group_ids.append(0 if token in token_group else 1)\n",
    "\n",
    "                # Add token to seen sequence\n",
    "                seen_sequence=torch.cat((seen_sequence,torch.tensor([token]).to(self.device)),dim=-1).to(self.device)\n",
    "\n",
    "            # Put the groups list into a list of bools\n",
    "            chunk_group_ids=[bool(int(x)) for x in chunk_group_ids]\n",
    "\n",
    "\n",
    "            # Now we have a full chunk_group_ids which represents a binary hash. This should be close to ideal hash generated by the private key when encoding (within allowed_distance)\n",
    "            # What we do next is check across similar hashes to the chunk_group_ids\n",
    "            outer_bool=False\n",
    "            for i in range(self.allowed_distance+1):\n",
    "\n",
    "                # This gets all possible combinations of ways we can index into a list i times\n",
    "                for x in itertools.combinations(range(self.token_standard),i):\n",
    "                    # For each one, copy the list, then change the values at each index\n",
    "                    copy_of_ids=chunk_group_ids.copy()\n",
    "\n",
    "                    for j in x:\n",
    "                        copy_of_ids[j]= not copy_of_ids[j]\n",
    "                    # Convert copy_of_ids into a binary string, then send it to a decimal\n",
    "                    encrypted_hash=int(\"\".join([str(int(x)) for x in copy_of_ids]),2)\n",
    "\n",
    "                    # Mod the hash with the public key 3rd prime\n",
    "                    encrypted_hash=encrypted_hash % self.r\n",
    "\n",
    "                    # Check this against the digest\n",
    "                    ret_bool=self.check_hash(encrypted_hash,digest)\n",
    "                    # If true, this chunk passes, so add that value to chunk bool and break both loops\n",
    "                    if ret_bool:\n",
    "                        chunk_bools.append(True)\n",
    "                        outer_bool=True\n",
    "                        break\n",
    "                if outer_bool:\n",
    "                    break\n",
    "\n",
    "            # If we didn't find a pass, add false to chunk_bools\n",
    "            if not outer_bool:\n",
    "                chunk_bools.append(False)\n",
    "\n",
    "\n",
    "            \n",
    "        \n",
    "        # Check if all chunks pass - all or all but one chunks must pass for sequence to pass detection\n",
    "        chunk_bools=np.array(chunk_bools,dtype=bool)\n",
    "        pass_bool=np.count_nonzero(chunk_bools==False)<2\n",
    "        return pass_bool,chunk_bools\n",
    "    \n",
    "    def check_hash(self,encrypted_hash,target_hash):\n",
    "\n",
    "        # Exponentiate the encrypted hash by the public key exponent, mod public key modulus, then take whole thing mod public key 3rd prime\n",
    "        decrypt=pow(encrypted_hash,self.e,self.n) % self.r\n",
    "        # If it matches the target_hash, return True, else return False\n",
    "        if decrypt==target_hash:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "token_standard=8 # length of token sequence chunks that we encrypt\n",
    "arb_size=32 # Bit Size of secret primes we create in the multi prime RSA implementation\n",
    "allowed_distance=3 # how much we allow generated chunks to differ from ideal chunks in terms of token group\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_key=(65537, 2113694530831295040457, 239) # generate public and private key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models from Hugging Face\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build encoder and decoder using the keys and params\n",
    "D=IdentityDetector(public_key,allowed_distance,token_standard,model,tokenizer,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signed prompt\n",
    "signed_prompt='The world is constantly changing due to technological advancements, which include the creation of powerful language models and advanced robotics technologies. A Computer Science degree can help one be involved in these changes and apply their knowledge to everyday life, as practical applications of technology.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This prompt was signed by the user\n",
      "This prompt passed detection for 5 / 5 chunk tests\n"
     ]
    }
   ],
   "source": [
    "# Does the encoded prompt pass? This should work >90% of the time.\n",
    "val,truth_vals= D.detect(signed_prompt) # val is overall pass/fail, truth_vals is pass/fail for each chunk of token_standard length in the prompt\n",
    "if val:\n",
    "    print(\"This prompt was signed by the user\")\n",
    "else:\n",
    "    print(\"This prompt was not signed by the user\")\n",
    "\n",
    "print(f\"This prompt passed detection for {np.count_nonzero(truth_vals)} / {truth_vals.size} chunk tests\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
